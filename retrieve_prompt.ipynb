{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we include some helper functions to interact with OpenAI's API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import client, model\n",
    "from src.utils import load_config\n",
    "from src.assistants.analyst.literature import literature_search\n",
    "import json\n",
    "\n",
    "def create_message_and_run_analyst_module(assistant, instructions, user_prompt, thread = None, save_tool_request_path = None):\n",
    "    '''\n",
    "    This function is a slightly simplified version of how the analyst module would be in WildfireGPT.\n",
    "    In this experiment, we only seek to understand how the changes in user profile affect the retrieved literature and the recommendations.\n",
    "    '''\n",
    "    if thread is None:\n",
    "        thread = client.beta.threads.create()\n",
    "    client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content=user_prompt\n",
    "    )\n",
    "    run = client.beta.threads.runs.create_and_poll(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id,\n",
    "        instructions=instructions,\n",
    "        parallel_tool_calls=False\n",
    "    )\n",
    "\n",
    "    tool_outputs = []\n",
    "\n",
    "    if run.required_action is not None:\n",
    "        # Loop through each tool in the required action section\n",
    "        for tool in run.required_action.submit_tool_outputs.tool_calls:\n",
    "            if tool.function.name == \"literature_search\":\n",
    "                function_args = json.loads(tool.function.arguments)\n",
    "                print(function_args)\n",
    "                if save_tool_request_path is not None:\n",
    "                    with open(save_tool_request_path, \"w\") as f:\n",
    "                        f.write(json.dumps(function_args))\n",
    "                function_response = literature_search(**function_args)\n",
    "                path = 'src/assistants/analyst/appendix/literature.md'\n",
    "                with open(path, \"r\") as f:\n",
    "                    appendix = f.read()\n",
    "                    function_response += appendix\n",
    "                tool_outputs.append({\n",
    "                    \"tool_call_id\": tool.id,\n",
    "                    \"output\": function_response\n",
    "                })\n",
    "\n",
    "        if len(tool_outputs) > 0:\n",
    "            run = client.beta.threads.runs.submit_tool_outputs(\n",
    "                thread_id=thread.id,\n",
    "                run_id=run.id,\n",
    "                tool_outputs=tool_outputs,\n",
    "            )\n",
    "\n",
    "    return thread\n",
    "    \n",
    "\n",
    "def generate_plan(profile):\n",
    "    '''\n",
    "    This function is a slightly simplified version of how the planning module would be in WildfireGPT.\n",
    "    In this experiment, we only seek to understand how the changes in user profile affect the retrieved literature and the recommendations.\n",
    "    '''\n",
    "    plan_config = load_config(\"src/assistants/plan/config.yml\")\n",
    "    plan_instruction = f\"{plan_config['instructions']}\\n{plan_config['available_datasets']}\\n{plan_config['example']}\\nHere is the information about your client: {profile}\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": plan_instruction},\n",
    "            {\"role\": \"user\", \"content\": \"Simply output the plan and nothing else.\"},\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "# There are five different user profiles that we will be using in this ablation experiment. \n",
    "\n",
    "\n",
    "template_profile = '''- **Profession:** {profession} in Virginia.\n",
    "- **Concern:** Managing the forest, keeping it healthy, while {concern}, and protecting properties from potential wildfires.\n",
    "- **Location:** Near Covington, VA with Latitude 37.7935 and Longitude -79.9939.\n",
    "- **Time:** Recommendations to be implemented within the next 5 to 10 years.\n",
    "- **Scope:** Management of the forest and properties to maximize {scope}, and protect against potential wildfires.\n",
    "'''\n",
    "\n",
    "profiles = {\n",
    "    \"homeowner\": {\n",
    "        \"profession\": \"Homeowner\",\n",
    "        \"concern\": \"maximizing marketable species\",\n",
    "        \"scope\": \"health and marketable species\",\n",
    "    },\n",
    "    \"civil_engineer\": \n",
    "    {\n",
    "        \"profession\": \"Civil Engineer\",\n",
    "        \"concern\": \"ensuring structural and infrastructural resilience\",\n",
    "        \"scope\": \"drainage efficiency and slope stability\",\n",
    "    },\n",
    "    \"ecologist\": \n",
    "    {\n",
    "        \"profession\": \"Ecologist\",\n",
    "        \"concern\": \"maintaining biodiversity and ecosystem services\",\n",
    "        \"scope\": \"ecological resilience and habitat connectivity\",\n",
    "    },\n",
    "    \"emergency_manager\": \n",
    "    {\n",
    "        \"profession\": \"Emergency Manager\",\n",
    "        \"concern\": \"establishing defendable space and evacuation corridors\",\n",
    "        \"scope\": \"emergency access and response capabilities\",\n",
    "    },\n",
    "    \"firefighter\": \n",
    "    {\n",
    "        \"profession\": \"Firefighter\",\n",
    "        \"concern\": \"establishing fuel breaks and maintaining tactical advantages\",\n",
    "        \"scope\": \"suppression effectiveness and crew safety\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below was the original instruction to the Analyst Module from the case study `case_studies/Private Property Protection`. \n",
    "\n",
    "In the ablation study below, we modify the user profile slightly to see how the model's predictions change. Specifically, we try to observe changes in \n",
    "- the plan proposed by the planning module with the updated user profile\n",
    "- the content of the literature review generated by the analyst module with the updated user profile and plan\n",
    "- the content of the recommendation generated by the analyst module with the updated user profile, plan, and literature review\n",
    "\n",
    "There are five different user profiles that we will be using in this ablation experiment. They are: homeowner, civil_engineer, ecologist, emergency_manager, and firefighter. You can see the minimal level of changes made to the user profile in the code block above.\n",
    "\n",
    "We will save everything under the folder `ablation_study` in the current working directory for reference and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_messages = client.beta.threads.messages.list(\"thread_KKbnKKvkPOFivz8r5ZAYPUCn\")\n",
    "assistant = client.beta.assistants.retrieve(thread_messages.data[0].assistant_id)\n",
    "print(assistant.instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now set the identify of the user\n",
    "user_identity = \"civil_engineer\"\n",
    "\n",
    "def generate_profile(profile):\n",
    "    profile_text = template_profile.format(**profile)\n",
    "    return profile_text\n",
    "\n",
    "profile = generate_profile(profiles[user_identity])\n",
    "print(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = generate_plan(profile)\n",
    "print(plan)\n",
    "\n",
    "import os\n",
    "if not os.path.exists(f\"ablation_study/{user_identity}\"):\n",
    "    os.makedirs(f\"ablation_study/{user_identity}\")\n",
    "\n",
    "with open(f\"ablation_study/{user_identity}/plan.md\", \"w\") as f:\n",
    "    f.write(plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "literature_search_user_prompt = \"Let's review some relevant literature.\"\n",
    "\n",
    "recommendation_user_prompt = \"Develop recommendations for species selection and planting strategies to enhance forest resilience.\"\n",
    "\n",
    "analyst_instruction = \"Here is the information about your client:\\n\\n{profile}\\n\\nHere is your overall plan to assist your client: {plan}\\n\\nHere is your client's last message: {user_prompt}\\n\\nHere is your plan for this step: {this_step}\\n\\nIf you give any recommendations, please provide the reasoning behind them and suggest the client to ask for supporting scientific evidence.\"\n",
    "\n",
    "this_step = {\n",
    "    'literature': \"Respond to the client's questions. Analyze literature. `literature_search`\", \n",
    "    'recommendation': \"Respond to the client's questions. Develop recommendations. `no tool needed`\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instrucion = analyst_instruction.format(profile=generate_profile(profiles[\"ecologist\"]), plan=plan, user_prompt=literature_search_user_prompt, this_step=this_step['literature'])\n",
    "\n",
    "thread = create_message_and_run_analyst_module(assistant, instrucion, literature_search_user_prompt, save_tool_request_path = f\"ablation_study/{user_identity}/literature_search_query.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(thread.id)\n",
    "# get last message from the thread\n",
    "thread_messages = client.beta.threads.messages.list(thread.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(thread_messages.data[0].content[0].text.value)\n",
    "\n",
    "# save the thread id to a file\n",
    "with open(f\"ablation_study/{user_identity}/thread_id.txt\", \"w\") as f:\n",
    "    f.write(thread.id)\n",
    "\n",
    "with open(f\"ablation_study/{user_identity}/literature_search_result.md\", \"w\") as f:\n",
    "    f.write(thread_messages.data[0].content[0].text.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instrucion = analyst_instruction.format(profile=generate_profile(profiles[\"ecologist\"]), plan=plan, user_prompt=recommendation_user_prompt, this_step=this_step['recommendation'])\n",
    "\n",
    "thread = create_message_and_run_analyst_module(assistant, instrucion, recommendation_user_prompt, thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(thread.id)\n",
    "# get last message from the thread\n",
    "thread_messages = client.beta.threads.messages.list(thread.id)\n",
    "print(thread_messages.data[0].content[0].text.value)\n",
    "\n",
    "with open(f\"ablation_study/{user_identity}/recommendation.md\", \"w\") as f:\n",
    "    f.write(thread_messages.data[0].content[0].text.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src-KW4cdEL7-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
